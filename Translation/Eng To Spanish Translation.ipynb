{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import typing\n",
    "from typing import Any, Tuple\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow_text as tf_text\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "#Paths\n",
    "DATA_PATH = 'lang'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "SPA_ENG_PATH = os.path.join(DATA_PATH, 'spa.txt')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "use_builtins = True"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "#Shape checker\n",
    "class ShapeChecker():\n",
    "  def __init__(self):\n",
    "    # Keep a cache of every axis-name seen\n",
    "    self.shapes = {}\n",
    "\n",
    "  def __call__(self, tensor, names, broadcast=False):\n",
    "    if not tf.executing_eagerly():\n",
    "      return\n",
    "\n",
    "    if isinstance(names, str):\n",
    "      names = (names,)\n",
    "\n",
    "    shape = tf.shape(tensor)\n",
    "    rank = tf.rank(tensor)\n",
    "\n",
    "    if rank != len(names):\n",
    "      raise ValueError(f'Rank mismatch:\\n'\n",
    "                       f'    found {rank}: {shape.numpy()}\\n'\n",
    "                       f'    expected {len(names)}: {names}\\n')\n",
    "\n",
    "    for i, name in enumerate(names):\n",
    "      if isinstance(name, int):\n",
    "        old_dim = name\n",
    "      else:\n",
    "        old_dim = self.shapes.get(name, None)\n",
    "      new_dim = shape[i]\n",
    "\n",
    "      if (broadcast and new_dim == 1):\n",
    "        continue\n",
    "\n",
    "      if old_dim is None:\n",
    "        # If the axis name is new, add its length to the cache.\n",
    "        self.shapes[name] = new_dim\n",
    "        continue\n",
    "\n",
    "      if new_dim != old_dim:\n",
    "        raise ValueError(f\"Shape mismatch for dimension: '{name}'\\n\"\n",
    "                         f\"    found: {new_dim}\\n\"\n",
    "                         f\"    expected: {old_dim}\\n\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Un día, me desperté y vi que Dios me había puesto pelo en la cara. Me lo afeité. Al día siguiente, vi que Dios me lo había vuelto a poner en la cara, así que me lo afeité otra vez. Al tercer día, cuando vi que Dios me había puesto pelo en la cara de nuevo, decidí que Dios se saliera con la suya. Por eso tengo barba.\n"
     ]
    }
   ],
   "source": [
    "def load_data(path):\n",
    "  file1 = open(path, encoding='utf-8')\n",
    "  text = file1.read()\n",
    "  lines = text.splitlines()\n",
    "  pairs = [line.split('\\t') for line in lines]\n",
    "  input = []\n",
    "  target = []\n",
    "  for x in pairs:\n",
    "    input.append(x[1])\n",
    "    target.append(x[0])\n",
    "  # inp = [inp for targ, inp in pairs]\n",
    "  # targ = [targ for targ, inp in pairs]\n",
    "\n",
    "  return target, input\n",
    "\n",
    "targ, inp = load_data(SPA_ENG_PATH)\n",
    "print(inp[-1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One day, I woke up to find that God had put hair on my face. I shaved it off. The next day, I found that God had put it back on my face, so I shaved it off again. On the third day, when I found that God had put hair back on my face again, I decided to let God have his way. That's why I have a beard.\n"
     ]
    }
   ],
   "source": [
    "print(targ[-1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create a tf.data dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(inp)\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((inp, targ)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([b'Tom va a necesitar ayuda.' b'Le ped\\xc3\\xad a Tom que se fuese.'], shape=(2,), dtype=string)\n",
      "\n",
      "tf.Tensor([b'Tom is going to need help.' b'I asked Tom to leave.'], shape=(2,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "for example_input_batch, example_target_batch in dataset.take(1):\n",
    "  print(example_input_batch[:2])\n",
    "  print()\n",
    "  print(example_target_batch[:2])\n",
    "  break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'\\xc2\\xbfTodav\\xc3\\xada est\\xc3\\xa1 en casa?'\n",
      "b'\\xc2\\xbfTodavi\\xcc\\x81a esta\\xcc\\x81 en casa?'\n"
     ]
    }
   ],
   "source": [
    "example_text = tf.constant('¿Todavía está en casa?')\n",
    "\n",
    "print(example_text.numpy())\n",
    "print(tf_text.normalize_utf8(example_text, 'NFKD').numpy())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Preprocessing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def tf_lower_and_split_punct(text):\n",
    "  # Split accecented characters.\n",
    "  text = tf_text.normalize_utf8(text, 'NFKD')\n",
    "  text = tf.strings.lower(text)\n",
    "  # Keep space, a to z, and select punctuation.\n",
    "  text = tf.strings.regex_replace(text, '[^ a-z.?!,¿]', '')\n",
    "  # Add spaces around punctuation.\n",
    "  text = tf.strings.regex_replace(text, '[.?!,¿]', r' \\0 ')\n",
    "  # Strip whitespace.\n",
    "  text = tf.strings.strip(text)\n",
    "\n",
    "  text = tf.strings.join(['[START]', text, '[END]'], separator=' ')\n",
    "  return text"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¿Todavía está en casa?\n",
      "[START] ¿ todavia esta en casa ? [END]\n"
     ]
    }
   ],
   "source": [
    "print(example_text.numpy().decode())\n",
    "print(tf_lower_and_split_punct(example_text).numpy().decode())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Text Vectorization"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "#Spanish text vectorizer\n",
    "max_vocab_size = 5000\n",
    "\n",
    "input_text_processor = tf.keras.layers.TextVectorization(\n",
    "    standardize=tf_lower_and_split_punct,\n",
    "    max_tokens=max_vocab_size)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "['', '[UNK]', '[START]', '[END]', '.', 'que', 'de', 'el', 'no', 'tom']"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text_processor.adapt(inp)\n",
    "\n",
    "# Here are the first 10 words from the vocabulary:\n",
    "input_text_processor.get_vocabulary()[:10]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "['', '[UNK]', '[START]', '[END]', '.', 'i', 'the', 'to', 'you', 'tom']"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#English text vectorizer\n",
    "output_text_processor = tf.keras.layers.TextVectorization(\n",
    "    standardize=tf_lower_and_split_punct,\n",
    "    max_tokens=max_vocab_size)\n",
    "\n",
    "output_text_processor.adapt(targ)\n",
    "output_text_processor.get_vocabulary()[:10]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Index out of range using input dim 1; input has only 1 dims [Op:StridedSlice] name: strided_slice/",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mInvalidArgumentError\u001B[0m                      Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_9056/3567523926.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[0mexample_tokens\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0minput_text_processor\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'hi'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 2\u001B[1;33m \u001B[0mexample_tokens\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;36m3\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m:\u001B[0m\u001B[1;36m10\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32m~\\Documents\\Python\\Anaconda\\Tensorflow Object Detection\\TFODCourse\\tfod\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001B[0m in \u001B[0;36mwrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    204\u001B[0m     \u001B[1;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    205\u001B[0m     \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 206\u001B[1;33m       \u001B[1;32mreturn\u001B[0m \u001B[0mtarget\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    207\u001B[0m     \u001B[1;32mexcept\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0mTypeError\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mValueError\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    208\u001B[0m       \u001B[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Documents\\Python\\Anaconda\\Tensorflow Object Detection\\TFODCourse\\tfod\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\u001B[0m in \u001B[0;36m_slice_helper\u001B[1;34m(tensor, slice_spec, var)\u001B[0m\n\u001B[0;32m   1039\u001B[0m       \u001B[0mvar_empty\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mconstant\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mdtypes\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mint32\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1040\u001B[0m       \u001B[0mpacked_begin\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mpacked_end\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mpacked_strides\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mvar_empty\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1041\u001B[1;33m     return strided_slice(\n\u001B[0m\u001B[0;32m   1042\u001B[0m         \u001B[0mtensor\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1043\u001B[0m         \u001B[0mpacked_begin\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Documents\\Python\\Anaconda\\Tensorflow Object Detection\\TFODCourse\\tfod\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001B[0m in \u001B[0;36mwrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    204\u001B[0m     \u001B[1;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    205\u001B[0m     \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 206\u001B[1;33m       \u001B[1;32mreturn\u001B[0m \u001B[0mtarget\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    207\u001B[0m     \u001B[1;32mexcept\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0mTypeError\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mValueError\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    208\u001B[0m       \u001B[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Documents\\Python\\Anaconda\\Tensorflow Object Detection\\TFODCourse\\tfod\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\u001B[0m in \u001B[0;36mstrided_slice\u001B[1;34m(input_, begin, end, strides, begin_mask, end_mask, ellipsis_mask, new_axis_mask, shrink_axis_mask, var, name)\u001B[0m\n\u001B[0;32m   1212\u001B[0m     \u001B[0mstrides\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mones_like\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mbegin\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1213\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1214\u001B[1;33m   op = gen_array_ops.strided_slice(\n\u001B[0m\u001B[0;32m   1215\u001B[0m       \u001B[0minput\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0minput_\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1216\u001B[0m       \u001B[0mbegin\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mbegin\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Documents\\Python\\Anaconda\\Tensorflow Object Detection\\TFODCourse\\tfod\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\u001B[0m in \u001B[0;36mstrided_slice\u001B[1;34m(input, begin, end, strides, begin_mask, end_mask, ellipsis_mask, new_axis_mask, shrink_axis_mask, name)\u001B[0m\n\u001B[0;32m  10508\u001B[0m       \u001B[1;32mreturn\u001B[0m \u001B[0m_result\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m  10509\u001B[0m     \u001B[1;32mexcept\u001B[0m \u001B[0m_core\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_NotOkStatusException\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m> 10510\u001B[1;33m       \u001B[0m_ops\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mraise_from_not_ok_status\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0me\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mname\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m  10511\u001B[0m     \u001B[1;32mexcept\u001B[0m \u001B[0m_core\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_FallbackException\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m  10512\u001B[0m       \u001B[1;32mpass\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Documents\\Python\\Anaconda\\Tensorflow Object Detection\\TFODCourse\\tfod\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001B[0m in \u001B[0;36mraise_from_not_ok_status\u001B[1;34m(e, name)\u001B[0m\n\u001B[0;32m   6939\u001B[0m   \u001B[0mmessage\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmessage\u001B[0m \u001B[1;33m+\u001B[0m \u001B[1;33m(\u001B[0m\u001B[1;34m\" name: \"\u001B[0m \u001B[1;33m+\u001B[0m \u001B[0mname\u001B[0m \u001B[1;32mif\u001B[0m \u001B[0mname\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32mNone\u001B[0m \u001B[1;32melse\u001B[0m \u001B[1;34m\"\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   6940\u001B[0m   \u001B[1;31m# pylint: disable=protected-access\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 6941\u001B[1;33m   \u001B[0msix\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mraise_from\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcore\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_status_to_exception\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0me\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcode\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmessage\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   6942\u001B[0m   \u001B[1;31m# pylint: enable=protected-access\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   6943\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Documents\\Python\\Anaconda\\Tensorflow Object Detection\\TFODCourse\\tfod\\lib\\site-packages\\six.py\u001B[0m in \u001B[0;36mraise_from\u001B[1;34m(value, from_value)\u001B[0m\n",
      "\u001B[1;31mInvalidArgumentError\u001B[0m: Index out of range using input dim 1; input has only 1 dims [Op:StridedSlice] name: strided_slice/"
     ]
    }
   ],
   "source": [
    "example_tokens = input_text_processor('hi')\n",
    "example_tokens[:3, :10]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'example_tokens' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_29332/3519548897.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[0mplt\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msubplot\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m2\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 2\u001B[1;33m \u001B[0mplt\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpcolormesh\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mexample_tokens\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      3\u001B[0m \u001B[0mplt\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtitle\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'Token IDs'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[0mplt\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msubplot\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m2\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m2\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'example_tokens' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.subplot(1, 2, 1)\n",
    "plt.pcolormesh(example_tokens)\n",
    "plt.title('Token IDs')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.pcolormesh(example_tokens != 0)\n",
    "plt.title('Mask')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "embedding_dim = 256\n",
    "units = 1024"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "  def __init__(self, input_vocab_size, embedding_dim, enc_units):\n",
    "    super(Encoder, self).__init__()\n",
    "    self.enc_units = enc_units\n",
    "    self.input_vocab_size = input_vocab_size\n",
    "\n",
    "    # The embedding layer converts tokens to vectors\n",
    "    self.embedding = tf.keras.layers.Embedding(self.input_vocab_size,\n",
    "                                               embedding_dim)\n",
    "\n",
    "    # The GRU RNN layer processes those vectors sequentially.\n",
    "    self.gru = tf.keras.layers.GRU(self.enc_units,\n",
    "                                   # Return the sequence and state\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "\n",
    "  def call(self, tokens, state=None):\n",
    "    shape_checker = ShapeChecker()\n",
    "    shape_checker(tokens, ('batch', 's'))\n",
    "\n",
    "    # 2. The embedding layer looks up the embedding for each token.\n",
    "    vectors = self.embedding(tokens)\n",
    "    shape_checker(vectors, ('batch', 's', 'embed_dim'))\n",
    "\n",
    "    # 3. The GRU processes the embedding sequence.\n",
    "    #    output shape: (batch, s, enc_units)\n",
    "    #    state shape: (batch, enc_units)\n",
    "    output, state = self.gru(vectors, initial_state=state)\n",
    "    shape_checker(output, ('batch', 's', 'enc_units'))\n",
    "    shape_checker(state, ('batch', 'enc_units'))\n",
    "\n",
    "    # 4. Returns the new sequence and its state.\n",
    "    return output, state"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Convert the input text to tokens.\n",
    "example_tokens = input_text_processor(example_input_batch)\n",
    "\n",
    "# Encode the input sequence.\n",
    "encoder = Encoder(input_text_processor.vocabulary_size(),\n",
    "                  embedding_dim, units)\n",
    "example_enc_output, example_enc_state = encoder(example_tokens)\n",
    "\n",
    "print(f'Input batch, shape (batch): {example_input_batch.shape}')\n",
    "print(f'Input batch tokens, shape (batch, s): {example_tokens.shape}')\n",
    "print(f'Encoder output, shape (batch, s, units): {example_enc_output.shape}')\n",
    "print(f'Encoder state, shape (batch, units): {example_enc_state.shape}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "  def __init__(self, units):\n",
    "    super().__init__()\n",
    "    # For Eqn. (4), the  Bahdanau attention\n",
    "    self.W1 = tf.keras.layers.Dense(units, use_bias=False)\n",
    "    self.W2 = tf.keras.layers.Dense(units, use_bias=False)\n",
    "\n",
    "    self.attention = tf.keras.layers.AdditiveAttention()\n",
    "\n",
    "  def call(self, query, value, mask):\n",
    "    shape_checker = ShapeChecker()\n",
    "    shape_checker(query, ('batch', 't', 'query_units'))\n",
    "    shape_checker(value, ('batch', 's', 'value_units'))\n",
    "    shape_checker(mask, ('batch', 's'))\n",
    "\n",
    "    # From Eqn. (4), `W1@ht`.\n",
    "    w1_query = self.W1(query)\n",
    "    shape_checker(w1_query, ('batch', 't', 'attn_units'))\n",
    "\n",
    "    # From Eqn. (4), `W2@hs`.\n",
    "    w2_key = self.W2(value)\n",
    "    shape_checker(w2_key, ('batch', 's', 'attn_units'))\n",
    "\n",
    "    query_mask = tf.ones(tf.shape(query)[:-1], dtype=bool)\n",
    "    value_mask = mask\n",
    "\n",
    "    context_vector, attention_weights = self.attention(\n",
    "        inputs = [w1_query, value, w2_key],\n",
    "        mask=[query_mask, value_mask],\n",
    "        return_attention_scores = True,\n",
    "    )\n",
    "    shape_checker(context_vector, ('batch', 't', 'value_units'))\n",
    "    shape_checker(attention_weights, ('batch', 't', 's'))\n",
    "\n",
    "    return context_vector, attention_weights"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "attention_layer = BahdanauAttention(units)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "(example_tokens != 0).shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Later, the decoder will generate this attention query\n",
    "example_attention_query = tf.random.normal(shape=[len(example_tokens), 2, 10])\n",
    "\n",
    "# Attend to the encoded tokens\n",
    "\n",
    "context_vector, attention_weights = attention_layer(\n",
    "    query=example_attention_query,\n",
    "    value=example_enc_output,\n",
    "    mask=(example_tokens != 0))\n",
    "\n",
    "print(f'Attention result shape: (batch_size, query_seq_length, units):           {context_vector.shape}')\n",
    "print(f'Attention weights shape: (batch_size, query_seq_length, value_seq_length): {attention_weights.shape}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.subplot(1, 2, 1)\n",
    "plt.pcolormesh(attention_weights[:, 0, :])\n",
    "plt.title('Attention weights')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.pcolormesh(example_tokens != 0)\n",
    "plt.title('Mask')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "attention_weights.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.suptitle('Attention weights for one sequence')\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "a1 = plt.subplot(1, 2, 1)\n",
    "plt.bar(range(len(attention_slice)), attention_slice)\n",
    "# freeze the xlim\n",
    "plt.xlim(plt.xlim())\n",
    "plt.xlabel('Attention weights')\n",
    "\n",
    "a2 = plt.subplot(1, 2, 2)\n",
    "plt.bar(range(len(attention_slice)), attention_slice)\n",
    "plt.xlabel('Attention weights, zoomed')\n",
    "\n",
    "# zoom in\n",
    "top = max(a1.get_ylim())\n",
    "zoom = 0.85*top\n",
    "a2.set_ylim([0.90*top, top])\n",
    "a1.plot(a1.get_xlim(), [zoom, zoom], color='k')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "  def __init__(self, output_vocab_size, embedding_dim, dec_units):\n",
    "    super(Decoder, self).__init__()\n",
    "    self.dec_units = dec_units\n",
    "    self.output_vocab_size = output_vocab_size\n",
    "    self.embedding_dim = embedding_dim\n",
    "\n",
    "    # For Step 1. The embedding layer convets token IDs to vectors\n",
    "    self.embedding = tf.keras.layers.Embedding(self.output_vocab_size,\n",
    "                                               embedding_dim)\n",
    "\n",
    "    # For Step 2. The RNN keeps track of what's been generated so far.\n",
    "    self.gru = tf.keras.layers.GRU(self.dec_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "\n",
    "    # For step 3. The RNN output will be the query for the attention layer.\n",
    "    self.attention = BahdanauAttention(self.dec_units)\n",
    "\n",
    "    # For step 4. Eqn. (3): converting `ct` to `at`\n",
    "    self.Wc = tf.keras.layers.Dense(dec_units, activation=tf.math.tanh,\n",
    "                                    use_bias=False)\n",
    "\n",
    "    # For step 5. This fully connected layer produces the logits for each\n",
    "    # output token.\n",
    "    self.fc = tf.keras.layers.Dense(self.output_vocab_size)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "tfod",
   "language": "python",
   "display_name": "tfod"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}